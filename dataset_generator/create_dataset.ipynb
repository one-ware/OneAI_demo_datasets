{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563d6805",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/one-ware/OneAI_demo_datasets/blob/main/dataset_generator/create_dataset.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd99a00",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Generator\n",
    "\n",
    "Generate synthetic training datasets by placing objects on backgrounds with automatic labeling.\n",
    "You can create object detection datasets, either single images or image pairs for reference detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afa686",
   "metadata": {},
   "source": [
    "## Quick Start Guide\n",
    "\n",
    "### Step 1: Choose Your Environment\n",
    "\n",
    "**Option A - Local Use:**\n",
    "- Skip cells marked with üåê (Google Colab only)\n",
    "- Use the configuration cell marked with üíª (Local)\n",
    "\n",
    "**Option B - Google Colab:**\n",
    "- Run cells marked with üåê (Google Colab)\n",
    "- Use the configuration cell marked with üåê (Colab)\n",
    "\n",
    "### Step 2: Prepare Your Images\n",
    "\n",
    "**Object Images (Required):**\n",
    "- Format: PNG with **transparent background**\n",
    "- Organize into folders by category (e.g., `/birds/`, `/drones/`)\n",
    "- Examples: bird.png, drone_01.png, etc.\n",
    "\n",
    "**Background Images (Required):**\n",
    "- Format: PNG or JPG\n",
    "- Place all in one folder\n",
    "- For video backgrounds: Name sequentially (e.g., `frame_0001.jpg`, `frame_0002.jpg`)\n",
    "\n",
    "### Step 3: Configure & Generate\n",
    "\n",
    "1. Set paths in the configuration cell\n",
    "2. Adjust parameters with sliders\n",
    "3. Run preview to check results\n",
    "4. Generate full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae864822",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Import necessary libraries and get images for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a62085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f048f9",
   "metadata": {},
   "source": [
    "#### üåê Google Colab Only: Video Frame Extraction\n",
    "\n",
    "*(Skip this if working locally or if you already have frame images)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, output_folder, frame_skip=5):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "\n",
    "    while cap.isOpened() and frame_count < 500:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % frame_skip == 0:\n",
    "            output_path = os.path.join(output_folder, f\"frame_{saved_count:04d}.jpg\")\n",
    "            cv2.imwrite(output_path, frame)\n",
    "            saved_count += 1\n",
    "        \n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {saved_count} frames to {output_folder}\")\n",
    "\n",
    "video_path = \"/content/video.mp4\"\n",
    "output_folder = \"/content/output_images\"\n",
    "extract_frames(video_path, output_folder, frame_skip=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12831448",
   "metadata": {},
   "source": [
    "#### üåê Google Colab Only: File Upload\n",
    "\n",
    "Upload your object and background images to Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåê Google Colab Only - Upload files\n",
    "# Skip this cell if working locally\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# After upload, organize files into folders:\n",
    "# !mkdir -p /content/objects/birds\n",
    "# !mkdir -p /content/objects/drones\n",
    "# !mkdir -p /content/backgrounds\n",
    "# Then move uploaded files to appropriate folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26264f6",
   "metadata": {},
   "source": [
    "#### üåê Google Colab Only: Download Dataset Generator\n",
    "\n",
    "Download the required Python module from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåê Google Colab Only\n",
    "# Local users: Make sure dataset_generator.py is in the same folder as this notebook\n",
    "\n",
    "import requests\n",
    "\n",
    "# Replace this with the actual raw URL of your Python file\n",
    "file_url = \"https://raw.githubusercontent.com/one-ware/OneAI_demo_datasets/refs/heads/main/dataset_generator/dataset_generator.py\"\n",
    "file_name = \"dataset_generator.py\" # Name you want to save the file as\n",
    "\n",
    "try:\n",
    "    response = requests.get(file_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(response.text)\n",
    "    print(f\"Successfully downloaded '{file_name}' from {file_url}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error downloading the file: {e}\")\n",
    "    print(\"Please ensure the URL is correct and accessible (e.g., raw.githubusercontent.com for GitHub files).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438794c6",
   "metadata": {},
   "source": [
    "## Import Required Functions\n",
    "\n",
    "Run this cell to load the dataset generator functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0da5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_generator import (\n",
    "    load_background_metadata, \n",
    "    preload_backgrounds, \n",
    "    get_image_dataset_from_folder, \n",
    "    create_dataset_from_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ab876",
   "metadata": {},
   "source": [
    "## 1. Configure Paths and Categories\n",
    "\n",
    "### Object Image Requirements:\n",
    "- Must be PNG format with **transparent background**\n",
    "- Organize into separate folders by category\n",
    "- Example structure:\n",
    "  ```\n",
    "  /objects/\n",
    "    /birds/\n",
    "      bird_01.png\n",
    "      bird_02.png\n",
    "    /drones/\n",
    "      drone_01.png\n",
    "      drone_02.png\n",
    "  ```\n",
    "  - start label ids with 1\n",
    "\n",
    "### Background Image Requirements:\n",
    "- PNG or JPG format\n",
    "- All in one folder\n",
    "- For video backgrounds: Use sequential naming (frame_0001.jpg, frame_0002.jpg, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7777ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your object categories and adjust paths accordingly\n",
    "categories = [\n",
    "    {\"name\": \"bird\", \"folder\": \"/content/bird\", \"class_id\": 1},\n",
    "    {\"name\": \"drone\", \"folder\": \"/content/drone\", \"class_id\": 2},\n",
    "]\n",
    "\n",
    "# Background folder\n",
    "background_folder = \"/content/bg\"\n",
    "\n",
    "# Output path\n",
    "output_path = \"./generated_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eedceec",
   "metadata": {},
   "source": [
    "### Step 2: Configure & Generate\n",
    "\n",
    "You can generate two types of datasets:\n",
    "\n",
    "1. **Reference Dataset (Overlay Mode)**:\n",
    "  - Generates **two images per sample**:\n",
    "    - **Reference Image**: Background only\n",
    "    - **Test Image**: Background with objects\n",
    "  - Useful for tasks like **change detection** or **object tracking**.\n",
    "  - Set the variable `is_overlay.value` to `True` to enable this mode.\n",
    "\n",
    "2. **Single Detection Dataset**:\n",
    "  - Generates **one image per sample**:\n",
    "    - **Detection Image**: Background with objects\n",
    "  - Useful for tasks like **object detection**.\n",
    "  - Set the variable `is_overlay.value` to `False` to use this mode.\n",
    "\n",
    "**How to Choose:**\n",
    "- Simply set `is_overlay` to `True` or `False` below to switch between these modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a477669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set if reference-test image dataset or single image detection dataset\n",
    "is_overlay = False  # Set to True for overlay mode, False for single image mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5656609",
   "metadata": {},
   "source": [
    "### 3. Set Parameters\n",
    "\n",
    "Adjust these parameters to control how objects are placed on the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99060de",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget_config = {\n",
    "    \"Object Parameters\": [\n",
    "        {\"type\": \"FloatRangeSlider\", \"name\": \"Object Size (cm)\", \"key\": \"object_size\", \"value\": [11.0, 15.0], \"min\": 1.0, \"max\": 50.0, \"step\": 0.5, \"description\": \"Minimum and maximum size of objects in centimeters.\"},\n",
    "        {\"type\": \"FloatRangeSlider\", \"name\": \"Rotation (¬∞)\", \"key\": \"rotation\", \"value\": [-1.0, 1.0], \"min\": -180.0, \"max\": 180.0, \"step\": 1.0, \"description\": \"Range of rotation angles for objects in degrees (e.g., -1¬∞ to 1¬∞ for slight rotation).\"},\n",
    "        {\"type\": \"IntRangeSlider\", \"name\": \"Objects/Image\", \"key\": \"num_objects\", \"value\": [1, 4], \"min\": 1, \"max\": 20, \"step\": 1, \"description\": \"Number of objects per image (e.g., 1 to 4 objects per image).\"},\n",
    "        {\"type\": \"FloatSlider\", \"name\": \"Min Distance (cm)\", \"key\": \"min_distance\", \"value\": 2.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.5, \"description\": \"Minimum distance between objects in centimeters to prevent crowding.\"},\n",
    "        {\"type\": \"Checkbox\", \"name\": \"Allow Overlap\", \"key\": \"allow_overlap\", \"value\": False, \"description\": \"Whether objects are allowed to overlap.\"},\n",
    "        {\"type\": \"FloatSlider\", \"name\": \"Max Overlap (%)\", \"key\": \"max_overlap\", \"value\": 0.1, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01, \"description\": \"Maximum allowable overlap between objects as a percentage of their area (0.0 to 1.0).\"},\n",
    "        {\"type\": \"Text\", \"name\": \"Category Weights\", \"key\": \"category_weights\", \"value\": \"1.0, 1.0\", \"description\": \"Comma-separated weights for each category (e.g., '1.0, 2.0' means category 2 is twice as likely to appear). Leave empty for equal weights.\"}\n",
    "    ],\n",
    "    \"Background Parameters\": [\n",
    "        {\"type\": \"FloatText\", \"name\": \"BG Width (cm)\", \"key\": \"bg_width\", \"value\": 150.0, \"description\": \"Default width of the background in centimeters.\"},\n",
    "        {\"type\": \"FloatRangeSlider\", \"name\": \"Placement Area Width (%)\", \"key\": \"placement_area_width\", \"value\": [0.05, 0.95], \"min\": 0.0, \"max\": 1.0, \"step\": 0.01, \"description\": \"Placement area as percentages of the background width (e.g., 5% to 95%).\"},\n",
    "        {\"type\": \"FloatRangeSlider\", \"name\": \"Placement Area Height (%)\", \"key\": \"placement_area_height\", \"value\": [0.05, 0.95], \"min\": 0.0, \"max\": 1.0, \"step\": 0.01, \"description\": \"Placement area as percentages of the background height (e.g., 5% to 95%).\"},\n",
    "        {\"type\": \"Checkbox\", \"name\": \"Video Background\", \"key\": \"is_video_background\", \"value\": True, \"description\": \"Whether the background is a video with different reference and test frames. Reference images can be one frame before/after. Deactivate for same reference and test background.\"}\n",
    "    ],\n",
    "    \"Placement Logic\": [\n",
    "        {\"type\": \"FloatSlider\", \"name\": \"Edge Margin\", \"key\": \"edge_margin\", \"value\": 0.1, \"min\": 0.0, \"max\": 0.5, \"step\": 0.05, \"description\": \"Margin to keep objects away from the edges (0.0 = no margin, 0.5 = large margin).\"},\n",
    "        {\"type\": \"FloatSlider\", \"name\": \"Center Bias\", \"key\": \"center_bias\", \"value\": 0.3, \"min\": 0.0, \"max\": 1.0, \"step\": 0.1, \"description\": \"Bias for placing objects closer to the center (0.0 = uniform, 1.0 = strongly favors center).\"}\n",
    "    ],\n",
    "    \"Blending and Visual Quality\": [\n",
    "        {\"type\": \"FloatSlider\", \"name\": \"Blending Strength\", \"key\": \"object_blending_strength\", \"value\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.05, \"description\": \"Strength of blending at object edges (0.0 = sharp edges, 1.0 = heavy blurring).\"},\n",
    "        {\"type\": \"IntText\", \"name\": \"Output Channels\", \"key\": \"output_channels\", \"value\": 3, \"description\": \"Number of color channels in output images (3 for RGB, 1 for grayscale).\"}\n",
    "    ],\n",
    "    \"Output Parameters\": [\n",
    "        {\"type\": \"IntText\", \"name\": \"Width (px)\", \"key\": \"output_width\", \"value\": 1080, \"description\": \"Width of the generated images in pixels.\"},\n",
    "        {\"type\": \"IntText\", \"name\": \"Height (px)\", \"key\": \"output_height\", \"value\": 720, \"description\": \"Height of the generated images in pixels.\"},\n",
    "        {\"type\": \"IntText\", \"name\": \"Total Images\", \"key\": \"num_images\", \"value\": 10, \"description\": \"Total number of images to generate in the dataset.\"},\n",
    "        {\"type\": \"IntText\", \"name\": \"Max Labels\", \"key\": \"max_labels\", \"value\": 10, \"description\": \"Maximum number of label boxes per image.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "def create_widgets_from_config(config):\n",
    "    widgets_dict = {}\n",
    "    widget_groups = []\n",
    "    \n",
    "    for group_name, params in config.items():\n",
    "        group_widgets = [widgets.HTML(f\"<h3 style='margin-bottom: 15px; color: #333;'>{group_name}</h3>\")]\n",
    "        for param in params:\n",
    "            widget_type = getattr(widgets, param[\"type\"])\n",
    "            # Set default step if not provided\n",
    "            step = param.get(\"step\", 1 if \"Int\" in param[\"type\"] else 0.1)\n",
    "            widget = widget_type(\n",
    "                value=param[\"value\"],\n",
    "                description=param[\"name\"],\n",
    "                min=param.get(\"min\"),\n",
    "                max=param.get(\"max\"),\n",
    "                step=step,\n",
    "                style={\"description_width\": \"150px\"},\n",
    "                layout=widgets.Layout(width=\"90%\"),\n",
    "            )\n",
    "            widgets_dict[param[\"key\"]] = widget\n",
    "            \n",
    "            # Add description below the widget with better styling\n",
    "            description_html = widgets.HTML(\n",
    "                f\"<div style='color: #666; font-size: 12px; margin-left: 160px; margin-top: -5px; margin-bottom: 5px; line-height: 1.4;'>\"\n",
    "                f\"{param.get('description', '')}</div>\"\n",
    "            )\n",
    "            \n",
    "            # Combine widget and description in a VBox\n",
    "            widget_with_desc = widgets.VBox(\n",
    "                [widget, description_html],\n",
    "                layout=widgets.Layout(margin=\"0 0 2px 0\")\n",
    "            )\n",
    "            group_widgets.append(widget_with_desc)\n",
    "        \n",
    "        widget_groups.append(widgets.VBox(\n",
    "            group_widgets, \n",
    "            layout=widgets.Layout(\n",
    "                width=\"90%\", \n",
    "                margin=\"5 0\",\n",
    "                padding=\"5px\",\n",
    "                border=\"1px solid #ddd\",\n",
    "                border_radius=\"5px\"\n",
    "            )\n",
    "        ))\n",
    "    \n",
    "    return widgets_dict, widget_groups\n",
    "\n",
    "# Create widgets\n",
    "widgets_dict, widget_groups = create_widgets_from_config(widget_config)\n",
    "\n",
    "# Display widget groups\n",
    "for group in widget_groups:\n",
    "    display(group)\n",
    "\n",
    "output = widgets.Output()\n",
    "save_button = widgets.Button(\n",
    "    description=\"Save Settings\",\n",
    "    button_style=\"success\",\n",
    "    icon=\"check\",\n",
    "    layout=widgets.Layout(width=\"200px\", height=\"40px\", margin=\"10px 0\")\n",
    ")\n",
    "\n",
    "def save_settings(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print(\"‚úì Settings saved successfully!\")\n",
    "\n",
    "save_button.on_click(save_settings)\n",
    "\n",
    "button_container = widgets.VBox(\n",
    "    [save_button, output],\n",
    "    layout=widgets.Layout(margin=\"20px 0\")\n",
    ")\n",
    "display(button_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372648f7",
   "metadata": {},
   "source": [
    "## 4. Preview\n",
    "\n",
    "**Generate 3 sample images** to verify your settings before creating the full dataset.\n",
    "\n",
    "The preview shows in overlay mode:\n",
    "- Left: Background image (reference frame)\n",
    "- Right: Background + objects (test frame) with bounding boxes\n",
    "else:\n",
    "- Image with objects and bounding boxes\n",
    "\n",
    "**Check:**\n",
    "- ‚úÖ Object sizes look realistic\n",
    "- ‚úÖ Objects are placed in good positions\n",
    "- ‚úÖ Bounding boxes are accurate\n",
    "- ‚úÖ Class labels are correct\n",
    "\n",
    "If something looks wrong, adjust parameters above and run this cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "background_metadata = load_background_metadata(\n",
    "    [background_folder],\n",
    "    default_bg_width_cm=widgets_dict[\"bg_width\"].value,\n",
    "    default_placement_area_pct=(widgets_dict[\"placement_area_width\"].value[0], widgets_dict[\"placement_area_height\"].value[0], widgets_dict[\"placement_area_width\"].value[1], widgets_dict[\"placement_area_height\"].value[1])\n",
    ")\n",
    "background_images, bg_widths, bg_placement_areas = preload_backgrounds(background_metadata)\n",
    "\n",
    "category_datasets = []\n",
    "category_class_ids = []\n",
    "for cat in categories:\n",
    "    ds = get_image_dataset_from_folder(cat[\"folder\"])\n",
    "    category_datasets.append(ds)\n",
    "    category_class_ids.append(cat[\"class_id\"])\n",
    "\n",
    "# Build parameters from widgets and configuration\n",
    "params = {\n",
    "    \"rotation_range\": widgets_dict[\"rotation\"].value,\n",
    "    \"number_of_objects_per_image\": widgets_dict[\"num_objects\"].value,\n",
    "    \"min_object_distance_cm\": widgets_dict[\"min_distance\"].value,\n",
    "    \"allow_overlap\": widgets_dict[\"allow_overlap\"].value,\n",
    "    \"max_overlap_pct\": widgets_dict[\"max_overlap\"].value,\n",
    "    \"default_bg_width_cm\": widgets_dict[\"bg_width\"].value,\n",
    "    \"default_placement_area_pct\": (\n",
    "        widgets_dict[\"placement_area_width\"].value[0], \n",
    "        widgets_dict[\"placement_area_height\"].value[0], \n",
    "        widgets_dict[\"placement_area_width\"].value[1], \n",
    "        widgets_dict[\"placement_area_height\"].value[1]\n",
    "    ),\n",
    "    \"is_video_background\": widgets_dict[\"is_video_background\"].value,\n",
    "    \"edge_avoidance\": widgets_dict[\"edge_margin\"].value,\n",
    "    \"prefer_center\": widgets_dict[\"center_bias\"].value,\n",
    "    \"output_width\": widgets_dict[\"output_width\"].value,\n",
    "    \"output_height\": widgets_dict[\"output_height\"].value,\n",
    "    \"num_images\": widgets_dict[\"num_images\"].value,\n",
    "    \"max_labels\": widgets_dict[\"max_labels\"].value,\n",
    "    \"is_overlay\": is_overlay,\n",
    "    \"object_blending_strength\": widgets_dict[\"object_blending_strength\"].value,\n",
    "    \"output_channels\": widgets_dict[\"output_channels\"].value\n",
    "}\n",
    "\n",
    "# Set min_scale_cm and max_scale_cm based on the object_size widget\n",
    "params[\"min_scale_cm\"] = widgets_dict[\"object_size\"].value[0]\n",
    "params[\"max_scale_cm\"] = widgets_dict[\"object_size\"].value[1]\n",
    "\n",
    "# Validate parameters\n",
    "if params[\"min_scale_cm\"] >= params[\"max_scale_cm\"]:\n",
    "    print(f\"Warning: min_scale_cm ({params['min_scale_cm']}) >= max_scale_cm ({params['max_scale_cm']})\")\n",
    "    print(\"Setting max_scale_cm to min_scale_cm + 1.0\")\n",
    "    params[\"max_scale_cm\"] = params[\"min_scale_cm\"] + 1.0\n",
    "\n",
    "if params[\"number_of_objects_per_image\"][0] > params[\"number_of_objects_per_image\"][1]:\n",
    "    print(f\"Warning: Invalid object range, fixing...\")\n",
    "    params[\"number_of_objects_per_image\"] = (1, max(2, params[\"number_of_objects_per_image\"][1]))\n",
    "\n",
    "# Parse category_weights from text input\n",
    "category_weights_str = widgets_dict[\"category_weights\"].value.strip()\n",
    "if category_weights_str:\n",
    "    try:\n",
    "        category_weights = [float(w.strip()) for w in category_weights_str.split(\",\")]\n",
    "        if len(category_weights) != len(categories):\n",
    "            print(f\"Warning: Number of weights ({len(category_weights)}) doesn't match number of categories ({len(categories)}). Using equal weights.\")\n",
    "            category_weights = None\n",
    "    except ValueError:\n",
    "        print(\"Warning: Invalid category weights format. Using equal weights.\")\n",
    "        category_weights = None\n",
    "else:\n",
    "    category_weights = None\n",
    "\n",
    "# Check channel corretness\n",
    "if params[\"output_channels\"] not in [1, 3]:\n",
    "    print(f\"Warning: output_channels ({params['output_channels']}) is not 1 or 3. Setting to 3.\")\n",
    "    params[\"output_channels\"] = 3\n",
    "\n",
    "params[\"category_weights\"] = category_weights\n",
    "\n",
    "# Generate preview\n",
    "preview_dataset = create_dataset_from_generator(\n",
    "    overlay=params['is_overlay'],\n",
    "    params=params,\n",
    "    background_images=background_images,\n",
    "    bg_widths=bg_widths,\n",
    "    bg_placement_areas=bg_placement_areas,\n",
    "    category_datasets=category_datasets,\n",
    "    category_class_ids=category_class_ids,\n",
    "    num_samples=3\n",
    ")\n",
    "\n",
    "# Display\n",
    "display_images = 2 if params['is_overlay'] else 1\n",
    "fig, axes = plt.subplots(3, display_images, figsize=(12, 16))\n",
    "\n",
    "for idx, (images, labels) in enumerate(preview_dataset.take(3)):\n",
    "    if params['is_overlay']:\n",
    "        axes[idx, 0].imshow(images[0].numpy())\n",
    "        axes[idx, 0].set_title(f'Background {idx+1}')\n",
    "        axes[idx, 0].axis('off')\n",
    "\n",
    "        axes[idx, 1].imshow(images[1].numpy())\n",
    "        axes[idx, 1].set_title(f'With Objects {idx+1}')\n",
    "        axes[idx, 1].axis('off')\n",
    "    else:\n",
    "        axes[idx].imshow(images.numpy())\n",
    "        axes[idx].set_title(f'Sample {idx+1}')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    # Draw bounding boxes (labels are now: exists, xmin, ymin, xmax, ymax, class_id in absolute pixels)\n",
    "    for label in labels:\n",
    "        exists, xmin, ymin, xmax, ymax, class_id = label.numpy()\n",
    "        if exists > 0.5:\n",
    "            # Coordinates are already in absolute pixels\n",
    "            box_w = xmax - xmin\n",
    "            box_h = ymax - ymin\n",
    "            \n",
    "            rect = plt.Rectangle((xmin, ymin), box_w, box_h,\n",
    "                                linewidth=2, edgecolor='lime', facecolor='none')\n",
    "            if params['is_overlay']:\n",
    "                axes[idx, 1].add_patch(rect)\n",
    "            else:\n",
    "                axes[idx].add_patch(rect)\n",
    "            \n",
    "            # Class label lookup by class_id\n",
    "            cat_name = next((c['name'] for c in categories if c['class_id'] == int(class_id)), f\"Class {int(class_id)}\")\n",
    "            if params['is_overlay']:\n",
    "                axes[idx, 1].text(\n",
    "                    xmin, ymin - 5, cat_name,\n",
    "                    color='lime', fontsize=10, weight='bold',\n",
    "                    bbox=dict(boxstyle='round', facecolor='black', alpha=0.5)\n",
    "                )\n",
    "            else:\n",
    "                axes[idx].text(\n",
    "                        xmin, ymin - 5, cat_name,\n",
    "                        color='lime', fontsize=10, weight='bold',\n",
    "                        bbox=dict(boxstyle='round', facecolor='black', alpha=0.5)\n",
    "                    )\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d60ac1",
   "metadata": {},
   "source": [
    "## 5. Generate Full Dataset\n",
    "\n",
    "**Only run this after the preview looks good!**\n",
    "\n",
    "This will generate all images and may take a while depending on the number of images.\n",
    "\n",
    "### Output Format:\n",
    "\n",
    "**Reference Dataset (Overlay Mode)**:\n",
    "- `image_XXXXX_temp.png`: Reference image (background only)\n",
    "- `image_XXXXX_test.png`: Test image (background + objects)\n",
    "- `image_XXXXX_test.txt`: Labels (bounding boxes)\n",
    "\n",
    "**Single Detection Dataset**:\n",
    "- `image_XXXXX.png`: Detection image (background + objects)\n",
    "- `image_XXXXX.txt`: Labels (bounding boxes)\n",
    "\n",
    "### Label Format (for OneAI):\n",
    "Each line in `.txt` file: `xmin ymin xmax ymax class_id`\n",
    "- Coordinates in absolute pixels\n",
    "- class_id matches your category configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be823a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the amount of images to generate\n",
    "num_images = 10  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0c8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "params[\"num_composite_images\"] = num_images\n",
    "print(f\"Generating {params['num_composite_images']} images...\")\n",
    "full_dataset = create_dataset_from_generator(\n",
    "    overlay=params['is_overlay'],\n",
    "    params=params,\n",
    "    background_images=background_images,\n",
    "    bg_widths=bg_widths,\n",
    "    bg_placement_areas=bg_placement_areas,\n",
    "    category_datasets=category_datasets,\n",
    "    category_class_ids=category_class_ids,\n",
    "    num_samples=params['num_composite_images']\n",
    ")\n",
    "\n",
    "# Save to disk\n",
    "export_path = Path(output_path)\n",
    "export_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Saving to {export_path}...\")\n",
    "for idx, (images, labels) in enumerate(full_dataset):\n",
    "    # Save images\n",
    "    if params['is_overlay']:\n",
    "        bg_img = (images[0].numpy() * 255).astype(np.uint8)\n",
    "        obj_img = (images[1].numpy() * 255).astype(np.uint8)\n",
    "        tf.io.write_file(str(export_path / f\"image_{idx:05d}_temp.png\"), tf.image.encode_png(bg_img))\n",
    "        tf.io.write_file(str(export_path / f\"image_{idx:05d}_test.png\"), tf.image.encode_png(obj_img))\n",
    "    else:\n",
    "        img = (images.numpy() * 255).astype(np.uint8)\n",
    "        tf.io.write_file(str(export_path / f\"image_{idx:05d}.png\"), tf.image.encode_png(img))\n",
    "    \n",
    "   \n",
    "    # Save labels in corner format: xmin ymin xmax ymax class_id (absolute pixels, 1-based)\n",
    "    if params['is_overlay']:\n",
    "        label_file = export_path / f\"image_{idx:05d}_test.txt\"\n",
    "    else:\n",
    "        label_file = export_path / f\"image_{idx:05d}.txt\"\n",
    "    with open(label_file, 'w') as f:\n",
    "        for label in labels:\n",
    "            exists, xmin, ymin, xmax, ymax, class_id = label.numpy()\n",
    "            if exists > 0.5:\n",
    "                # Write in format: xmin ymin xmax ymax class_id (absolute pixel coordinates)\n",
    "                f.write(f\"{int(xmin)} {int(ymin)} {int(xmax)} {int(ymax)} {int(class_id)}\\n\")\n",
    "    \n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"  {idx + 1}/{params['num_composite_images']} images saved...\")\n",
    "\n",
    "print(f\"‚úì Done! {params['num_composite_images']} images saved to {export_path}\")\n",
    "print(f\"Label format: xmin ymin xmax ymax class_id (absolute pixel coordinates, 1-based class IDs)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oneai-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
